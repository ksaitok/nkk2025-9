<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>日系解体工業株式会社 - 湘南・横浜の戸建解体</title>
    <meta name="description" content="湘南・横浜の戸建解体工事は日系解体工業株式会社へ。年間300件の実績。自社施工だから余計なマージンが無い安心価格。">
    <link rel="stylesheet" href="assets/css/main.css">
    <style>
        /* Language switcher functionality for static HTML */
        .language-content { display: none; }
        .language-content.active { display: block; }
        
        /* Additional styles for language buttons */
        .lang-switcher {
            position: fixed;
            top: 10px;
            right: 10px;
            z-index: 1000;
            display: flex;
            gap: 5px;
        }
        
        .lang-btn {
            padding: 8px 12px;
            background: #f0f0f0;
            border: 1px solid #ddd;
            border-radius: 3px;
            cursor: pointer;
            font-size: 12px;
            text-decoration: none;
            color: #333;
        }
        
        .lang-btn:hover {
            background: #e0e0e0;
        }
        
        .lang-btn.active {
            background: #d32f2f;
            color: white;
        }
    </style>
</head>
<body>
    <!-- Language Switcher -->
    <div class="lang-switcher">
        <button class="lang-btn active" onclick="switchLang('ja')">日本語</button>
        <button class="lang-btn" onclick="switchLang('en')">English</button>
        <button class="lang-btn" onclick="switchLang('pt')">Português</button>
    </div>

    <!-- Header -->
    <header class="main-header">
        <div class="container">
            <div class="header-content">
                <div class="logo">
                    <h1>日系解体工業株式会社</h1>
                </div>
                <nav class="main-nav">
                    <ul>
                        <li><a href="#home">ホーム</a></li>
                        <li><a href="#services">サービス</a></li>
                        <li><a href="#demolition-history">解体実績</a></li>
                        <li><a href="#company">会社概要</a></li>
                        <li><a href="#contact">お問い合わせ</a></li>
                    </ul>
                </nav>
            </div>
        </div>
    </header>

    <!-- Japanese Content -->
    <div id="ja-content" class="language-content active">
        <!-- Hero Section -->
        <section id="home" class="hero-section">
            <div class="container">
                <div class="hero-content">
                    <h2>湘南・横浜の戸建解体は日系解体工業株式会社へ</h2>
                    <p>戸建の解体なら日系解体へ「安全な施工・安心の価格」どんな構造、立地でも、丁寧な工事をお約束します！</p>
                    <a href="#contact" class="cta-button">お問い合わせ</a>
                </div>
            </div>
        </section>

        <!-- Services Section -->
        <section id="services" class="services-section">
            <div class="container">
                <h2>対応可能な工事</h2>
                <div class="services-grid">
                    <div class="service-card">
                        <h3>RC造</h3>
                        <p>鉄筋コンクリート造の建物解体</p>
                    </div>
                    <div class="service-card">
                        <h3>木造</h3>
                        <p>木造建築物の解体工事</p>
                    </div>
                    <div class="service-card">
                        <h3>鉄骨造</h3>
                        <p>鉄骨構造の建物解体</p>
                    </div>
                    <div class="service-card">
                        <h3>内装解体</h3>
                        <p>室内の内装解体工事</p>
                    </div>
                    <div class="service-card">
                        <h3>樹木伐採</h3>
                        <p>樹木の伐採作業</p>
                    </div>
                    <div class="service-card">
                        <h3>火災家屋</h3>
                        <p>火災被害建物の解体</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Company Info -->
        <section id="company" class="company-section">
            <div class="container">
                <h2>会社概要</h2>
                <div class="company-info">
                    <p><strong>会社名:</strong> 日系解体工業株式会社</p>
                    <p><strong>住所:</strong> 神奈川県茅ヶ崎市下寺尾1759</p>
                    <p><strong>電話:</strong> 0467-38-8842</p>
                    <p><strong>FAX:</strong> 0467-38-8843</p>
                    <p><strong>Email:</strong> momose2201@yahoo.co.jp</p>
                </div>
            </div>
        </section>
    </div>

    <!-- English Content -->
    <div id="en-content" class="language-content">
        <!-- Hero Section -->
        <section id="home" class="hero-section">
            <div class="container">
                <div class="hero-content">
                    <h2>House Demolition in Shonan/Yokohama - Nikkei Demolition Co., Ltd.</h2>
                    <p>For house demolition, trust Nikkei Demolition - "Safe construction, reliable prices" We promise careful work for any structure or location!</p>
                    <a href="#contact" class="cta-button">Contact Us</a>
                </div>
            </div>
        </section>

        <!-- Services Section -->
        <section id="services" class="services-section">
            <div class="container">
                <h2>Available Services</h2>
                <div class="services-grid">
                    <div class="service-card">
                        <h3>RC Structures</h3>
                        <p>Reinforced concrete building demolition</p>
                    </div>
                    <div class="service-card">
                        <h3>Wooden Structures</h3>
                        <p>Wooden building demolition work</p>
                    </div>
                    <div class="service-card">
                        <h3>Steel Structures</h3>
                        <p>Steel structure building demolition</p>
                    </div>
                    <div class="service-card">
                        <h3>Interior Demolition</h3>
                        <p>Interior demolition work</p>
                    </div>
                    <div class="service-card">
                        <h3>Tree Cutting</h3>
                        <p>Tree cutting and removal services</p>
                    </div>
                    <div class="service-card">
                        <h3>Fire Damage</h3>
                        <p>Fire-damaged building demolition</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Company Info -->
        <section id="company" class="company-section">
            <div class="container">
                <h2>Company Information</h2>
                <div class="company-info">
                    <p><strong>Company Name:</strong> Nikkei Demolition Co., Ltd.</p>
                    <p><strong>Address:</strong> 1759 Shitaderao, Chigasaki City, Kanagawa Prefecture</p>
                    <p><strong>Phone:</strong> 0467-38-8842</p>
                    <p><strong>FAX:</strong> 0467-38-8843</p>
                    <p><strong>Email:</strong> momose2201@yahoo.co.jp</p>
                </div>
            </div>
        </section>
    </div>

    <!-- Portuguese Content -->
    <div id="pt-content" class="language-content">
        <!-- Hero Section -->
        <section id="home" class="hero-section">
            <div class="container">
                <div class="hero-content">
                    <h2>Demolição de Casas em Shonan/Yokohama - Nikkei Demolição Ltda.</h2>
                    <p>Para demolição de casas, confie na Nikkei Demolição - "Construção segura, preços confiáveis" Prometemos trabalho cuidadoso para qualquer estrutura ou localização!</p>
                    <a href="#contact" class="cta-button">Contato</a>
                </div>
            </div>
        </section>

        <!-- Services Section -->
        <section id="services" class="services-section">
            <div class="container">
                <h2>Serviços Disponíveis</h2>
                <div class="services-grid">
                    <div class="service-card">
                        <h3>Estruturas de RC</h3>
                        <p>Demolição de edifícios de concreto armado</p>
                    </div>
                    <div class="service-card">
                        <h3>Estruturas de Madeira</h3>
                        <p>Trabalho de demolição de edifícios de madeira</p>
                    </div>
                    <div class="service-card">
                        <h3>Estruturas de Aço</h3>
                        <p>Demolição de edifícios com estrutura de aço</p>
                    </div>
                    <div class="service-card">
                        <h3>Demolição Interior</h3>
                        <p>Trabalho de demolição interior</p>

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Language Switcher -->
    <div class="lang-switcher">
        <button class="lang-btn active" onclick="switchLang('ja')">日本語</button>
        <button class="lang-btn" onclick="switchLang('en')">English</button>
        <button class="lang-btn" onclick="switchLang('pt')">Português</button>
    </div>

    <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Header -->
    <header class="main-header">
        <div class="container">
            <div class="header-content">
                <div class="logo">
                    <h1>日系解体工業株式会社</h1>
                </div>
                <nav class="main-nav">
                    <ul>
                        <li><a href="#home">ホーム</a></li>
                        <li><a href="#services">サービス</a></li>
                        <li><a href="#demolition-history">解体実績</a></li>
                        <li><a href="#company">会社概要</a></li>
                        <li><a href="#contact">お問い合わせ</a></li>
                    </ul>
                </nav>
            </div>
        </div>
    </header>

    <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Japanese Content -->
    <div id="ja-content" class="language-content active">
        <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Hero Section -->
        <section id="home" class="hero-section">
            <div class="container">
                <div class="hero-content">
                    <h2>湘南・横浜の戸建解体は日系解体工業株式会社へ</h2>
                    <p>戸建の解体なら日系解体へ「安全な施工・安心の価格」どんな構造、立地でも、丁寧な工事をお約束します！</p>
                    <a href="#contact" class="cta-button">お問い合わせ</a>
                </div>
            </div>
        </section>

        <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Services Section -->
        <section id="services" class="services-section">
            <div class="container">
                <h2>対応可能な工事</h2>
                <div class="services-grid">
                    <div class="service-card">
                        <h3>RC造</h3>
                        <p>鉄筋コンクリート造の建物解体</p>
                    </div>
                    <div class="service-card">
                        <h3>木造</h3>
                        <p>木造建築物の解体工事</p>
                    </div>
                    <div class="service-card">
                        <h3>鉄骨造</h3>
                        <p>鉄骨構造の建物解体</p>
                    </div>
                    <div class="service-card">
                        <h3>内装解体</h3>
                        <p>室内の内装解体工事</p>
                    </div>
                    <div class="service-card">
                        <h3>樹木伐採</h3>
                        <p>樹木の伐採作業</p>
                    </div>
                    <div class="service-card">
                        <h3>火災家屋</h3>
                        <p>火災被害建物の解体</p>
                    </div>
                </div>
            </div>
        </section>

        <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Company Info -->
        <section id="company" class="company-section">
            <div class="container">
                <h2>会社概要</h2>
                <div class="company-info">
                    <p><strong>会社名:</strong> 日系解体工業株式会社</p>
                    <p><strong>住所:</strong> 神奈川県茅ヶ崎市下寺尾1759</p>
                    <p><strong>電話:</strong> 0467-38-8842</p>
                    <p><strong>FAX:</strong> 0467-38-8843</p>
                    <p><strong>Email:</strong> momose2201@yahoo.co.jp</p>
                </div>
            </div>
        </section>
    </div>

    <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 English Content -->
    <div id="en-content" class="language-content">
        <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Hero Section -->
        <section id="home" class="hero-section">
            <div class="container">
                <div class="hero-content">
                    <h2>House Demolition in Shonan/Yokohama - Nikkei Demolition Co., Ltd.</h2>
                    <p>For house demolition, trust Nikkei Demolition - "Safe construction, reliable prices" We promise careful work for any structure or location!</p>
                    <a href="#contact" class="cta-button">Contact Us</a>
                </div>
            </div>
        </section>

        <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Services Section -->
        <section id="services" class="services-section">
            <div class="container">
                <h2>Available Services</h2>
                <div class="services-grid">
                    <div class="service-card">
                        <h3>RC Structures</h3>
                        <p>Reinforced concrete building demolition</p>
                    </div>
                    <div class="service-card">
                        <h3>Wooden Structures</h3>
                        <p>Wooden building demolition work</p>
                    </div>
                    <div class="service-card">
                        <h3>Steel Structures</h3>
                        <p>Steel structure building demolition</p>
                    </div>
                    <div class="service-card">
                        <h3>Interior Demolition</h3>
                        <p>Interior demolition work</p>
                    </div>
                    <div class="service-card">
                        <h3>Tree Cutting</h3>
                        <p>Tree cutting and removal services</p>
                    </div>
                    <div class="service-card">
                        <h3>Fire Damage</h3>
                        <p>Fire-damaged building demolition</p>
                    </div>
                </div>
            </div>
        </section>

        <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Company Info -->
        <section id="company" class="company-section">
            <div class="container">
                <h2>Company Information</h2>
                <div class="company-info">
                    <p><strong>Company Name:</strong> Nikkei Demolition Co., Ltd.</p>
                    <p><strong>Address:</strong> 1759 Shitaderao, Chigasaki City, Kanagawa Prefecture</p>
                    <p><strong>Phone:</strong> 0467-38-8842</p>
                    <p><strong>FAX:</strong> 0467-38-8843</p>
                    <p><strong>Email:</strong> momose2201@yahoo.co.jp</p>
                </div>
            </div>
        </section>
    </div>

    <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Portuguese Content -->
    <div id="pt-content" class="language-content">
        <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Hero Section -->
        <section id="home" class="hero-section">
            <div class="container">
                <div class="hero-content">
                    <h2>Demolição de Casas em Shonan/Yokohama - Nikkei Demolição Ltda.</h2>
                    <p>Para demolição de casas, confie na Nikkei Demolição - "Construção segura, preços confiáveis" Prometemos trabalho cuidadoso para qualquer estrutura ou localização!</p>
                    <a href="#contact" class="cta-button">Contato</a>
                </div>
            </div>
        </section>

        <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Services Section -->
        <section id="services" class="services-section">
            <div class="container">
                <h2>Serviços Disponíveis</h2>
                <div class="services-grid">
                    <div class="service-card">
                        <h3>Estruturas de RC</h3>
                        <p>Demolição de edifícios de concreto armado</p>
                    </div>
                    <div class="service-card">
                        <h3>Estruturas de Madeira</h3>
                        <p>Trabalho de demolição de edifícios de madeira</p>
                    </div>
                    <div class="service-card">
                        <h3>Estruturas de Aço</h3>
                        <p>Demolição de edifícios com estrutura de aço</p>
                    </div>
                    <div class="service-card">
                        <h3>Demolição Interior</h3>
                        <p>Trabalho de demolição interior</p>
                    </div>
                    <div class="service-card">
                        <h3>Corte de Árvores</h3>
                        <p>Serviços de corte e remoção de árvores</p>
                    </div>
                    <div class="service-card">
                        <h3>Danos por Fogo</h3>
                        <p>Demolição de edifícios danificados por fogo</p>
                    </div>
                </div>
            </div>
        </section>

        <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Company Info -->
        <section id="company" class="company-section">
            <div class="container">
                <h2>Informações da Empresa</h2>
                <div class="company-info">
                    <p><strong>Nome da Empresa:</strong> Nikkei Demolição Ltda.</p>
                    <p><strong>Endereço:</strong> 1759 Shitaderao, Cidade Chigasaki, Prefeitura de Kanagawa</p>
                    <p><strong>Telefone:</strong> 0467-38-8842</p>
                    <p><strong>FAX:</strong> 0467-38-8843</p>
                    <p><strong>Email:</strong> momose2201@yahoo.co.jp</p>
                </div>
            </div>
        </section>
    </div>

    <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Footer -->
    <footer class="main-footer">
        <div class="container">
            <p>&copy; 2024 日系解体工業株式会社 All Rights Reserved.</p>
            <p><a href="admin/login.php">管理者ログイン</a></p>
        </div>
    </footer>

    <script>
        // Language switching function
        function switchLang(lang) {
            // Hide all language content
            document.querySelectorAll('.language-content').forEach(content => {
                content.classList.remove('active');
            });
            
            // Remove active class from all buttons
            document.querySelectorAll('.lang-btn').forEach(btn => {
                btn.classList.remove('active');
            });
            
            // Show selected language content
            document.getElementById(lang + '-content').classList.add('active');
            
            // Add active class to clicked button
            event.target.classList.add('active');
        }
        
        // Smooth scrolling for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });
        
        // Add scroll to top functionality
        window.addEventListener('scroll', function() {
            if (window.pageYOffset > 300) {
                if (!document.querySelector('.scroll-to-top')) {
                    const button = document.createElement('button');
                    button.innerHTML = '↑';
                    button.className = 'scroll-to-top';
                    button.style.cssText = `
                        position: fixed;
                        bottom: 20px;
                        right: 20px;
                        background: #d32f2f;
                        color: white;
                        border: none;
                        border-radius: 50%;
                        width: 50px;
                        height: 50px;
                        font-size: 20px;
                        cursor: pointer;
                        z-index: 1000;
                    `;
                    button.addEventListener('click', function() {
                        window.scrollTo({ top: 0, behavior: 'smooth' });
                    });
                    document.body.appendChild(button);
                }
            } else {
                const button = document.querySelector('.scroll-to-top');
                if (button) {
                    button.remove();
                }
            }
        });
    </script>
</body>
</html>

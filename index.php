<?php
// Main entry point for Nikkei Kaitai website
require_once 'config/config.php';

// Get current language
$current_lang = getCurrentLanguage();

// Load language file
$translations = [];
$lang_file = "languages/{$current_lang}.json";
if (file_exists($lang_file)) {
    $translations = json_decode(file_get_contents($lang_file), true);
}

// Helper function to get translation
function t($key, $default = '') {
    global $translations;
    $keys = explode('.', $key);
    $value = $translations;
    
    foreach ($keys as $k) {
        if (isset($value[$k])) {
            $value = $value[$k];
        } else {
            return $default ?: $key;
        }
    }
    
    return $value;
}

// Set page variables
$page_title = t('site.title');
$page_description = t('site.description');
$active_page = 'home';
?>
<!DOCTYPE html>
<html lang="<?php echo $current_lang; ?>">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title><?php echo $page_title; ?></title>
    <meta name="description" content="<?php echo $page_description; ?>">
    <link rel="stylesheet" href="assets/css/main.css">
    <link rel="stylesheet" href="assets/css/responsive.css">
</head>
<body>
    <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Language Switcher -->
    <div class="language-switcher">
        <a href="?lang=ja" class="lang-btn <?php echo $current_lang === 'ja' ? 'active' : ''; ?>">日本語</a>
        <a href="?lang=en" class="lang-btn <?php echo $current_lang === 'en' ? 'active' : ''; ?>">English</a>
        <a href="?lang=pt" class="lang-btn <?php echo $current_lang === 'pt' ? 'active' : ''; ?>">Português</a>
    </div>

    <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Header -->
    <header class="main-header">
        <div class="container">
            <div class="header-content">
                <div class="logo">
                    <h1><?php echo t('site.title'); ?></h1>
                </div>
                <nav class="main-nav">
                    <ul>
                        <li><a href="index.php"><?php echo t('nav.home'); ?></a></li>
                        <li><a href="services.php"><?php echo t('nav.services'); ?></a></li>
                        <li><a href="demolition-history.php"><?php echo t('nav.demolition_history'); ?></a></li>
                        <li><a href="company.php"><?php echo t('nav.company'); ?></a></li>
                        <li><a href="contact.php"><?php echo t('nav.contact'); ?></a></li>
                    </ul>
                </nav>
            </div>
        </div>
    </header>

    <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Hero Section -->
    <section class="hero-section">
        <div class="container">
            <div class="hero-content">
                <h2><?php echo t('home.main_title'); ?></h2>
                <p><?php echo t('home.subtitle'); ?></p>
                <a href="contact.php" class="cta-button">お問い合わせ</a>
            </div>
        </div>
    </section>

    <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Services Section -->
    <section class="services-section">
        <div class="container">
            <h2>対応可能な工事</h2>
            <div class="services-grid">
                <div class="service-card">
                    <h3>RC造</h3>
                    <p>鉄筋コンクリート造の建物解体</p>
                </div>
                <div class="service-card">
                    <h3>木造</h3>
                    <p>木造建築物の解体工事</p>
                </div>
                <div class="service-card">
                    <h3>鉄骨造</h3>
                    <p>鉄骨構造の建物解体</p>
                </div>
                <div class="service-card">
                    <h3>内装解体</h3>
                    <p>室内の内装解体工事</p>
                </div>
                <div class="service-card">
                    <h3>樹木伐採</h3>
                    <p>樹木の伐採作業</p>
                </div>
                <div class="service-card">
                    <h3>火災家屋</h3>
                    <p>火災被害建物の解体</p>
                </div>
            </div>
        </div>
    </section>

    <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Company Info -->
    <section class="company-section">
        <div class="container">
            <h2>会社概要</h2>
            <div class="company-info">
                <p><strong>会社名:</strong> 日系解体工業株式会社</p>
                <p><strong>住所:</strong> 神奈川県茅ヶ崎市下寺尾1759</p>
                <p><strong>電話:</strong> 0467-38-8842</p>
                <p><strong>FAX:</strong> 0467-38-8843</p>
                <p><strong>Email:</strong> momose2201@yahoo.co.jp</p>
            </div>
        </div>
    </section>

    <---
title: "Developing RAG Systems with DeepSeek R1 & Ollama (Complete Code Included)"
source: "https://sebastian-petrus.medium.com/developing-rag-systems-with-deepseek-r1-ollama-f2f561cfda97"
author:
  - "[[Sebastian Petrus]]"
published: 2025-01-24
created: 2025-01-26
description: "Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a Retrieval-Augmented Generation (RAG) system using DeepSeek R1, an open-source…"
tags:
  - "clippings"
---
## Resumo
Este artigo detalha como construir um sistema RAG (Retrieval-Augmented Generation) usando DeepSeek R1 e Ollama. Principais pontos:

- DeepSeek R1 é uma alternativa 95% mais barata ao OpenAI, com foco em recuperação precisa usando apenas 3 chunks de documento por resposta
- O sistema usa Ollama para execução local de modelos, eliminando latência de API em nuvem
- Implementação passo a passo inclui:
  - Uso de LangChain para processamento de documentos
  - Streamlit para interface web
  - PDFPlumberLoader para extração de texto
  - Chunking semântico de documentos
  - Criação de base de conhecimento pesquisável com FAISS
  - Configuração do modelo DeepSeek R1 1.5B
  - Montagem da pipeline RAG completa

- O código completo é fornecido e explicado em detalhes
- Benefícios incluem:
  - Processamento local
  - Respostas precisas baseadas apenas no contexto fornecido
  - Interface web amigável
  - Capacidade de fazer perguntas diretamente a PDFs

O artigo termina discutindo o futuro do RAG com DeepSeek, mencionando recursos futuros como auto-verificação e raciocínio multi-hop.
*(Summary generated by GPT) *

# Content
[

Petrus](https://miro.medium.com/v2/resize:fill:88:88/1*jRkLFadAFjkcyEN5bCnVZA.png)

](https://sebastian-petrus.medium.com/?source=post_page---byline--f2f561cfda97--------------------------------)

Ever wished you could directly ask questions to a PDF or technical manual? This guide will show you how to build a **Retrieval-Augmented Generation (RAG)** system using **DeepSeek R1**, an open-source reasoning tool, and **Ollama**, a lightweight framework for running local AI models.

## Pro tip: Streamline API Testing with Apidog

![](https://miro.medium.com/v2/resize:fit:889/1*rTJXS7c5Aq2XWixkb4tqKQ.png)

Looking to simplify your API workflows? **Apidog** acts as an all-in-one solution for creating, managing, and running tests and mock servers. With Apidog, you can:

- Automate critical workflows without juggling multiple tools or writing extensive scripts.
- Maintain smooth CI/CD pipelines.
- Identify bottlenecks to ensure API reliability.

Save time and focus on perfecting your product. Ready to try it? [Give **Apidog** a spin](https://bit.ly/3Teeyxv)!

## Why DeepSeek R1?

**DeepSeek R1**, a model comparable to OpenAI’s o1 but 95% cheaper, is revolutionizing RAG systems. Developers love it for its:

- **Focused retrieval**: Uses only 3 document chunks per answer.
- **Strict prompting**: Avoids hallucinations with an “I don’t know” response.
- **Local execution**: Eliminates cloud API latency.

## What You’ll Need to Build a Local RAG System

## 1\. Ollama

Ollama lets you run models like DeepSeek R1 locally.

- **Download:** [Ollama](https://ollama.com/)
- **Setup:** Install and run the following command via your terminal.

```
ollama run deepseek-r1  
```
![Ollama homepage](https://miro.medium.com/v2/resize:fit:889/0*N3tTKqa1xRJOO2iP.png)

## 2\. DeepSeek R1 Model Variants

DeepSeek R1 ranges from 1.5B to 671B parameters. Start small with the **1.5B model** for lightweight RAG applications.

```
ollama run deepseek-r1:1.5b
```

*Pro Tip*: Larger models (e.g., 70B) offer better reasoning but need more RAM.

![](https://miro.medium.com/v2/resize:fit:889/0*9SxSxn50QlG-p_2H.png)

## Step-by-Step Guide to Building the RAG Pipeline

## Step 1: Import Libraries

We’ll use:

- [**LangChain**](https://github.com/langchain-ai/langchain) for document processing and retrieval.
- [**Streamlit**](https://streamlit.io/) for the user-friendly web interface.

```
import streamlit as st  
from langchain_community.document_loaders import PDFPlumberLoader  
from langchain_experimental.text_splitter import SemanticChunker  
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS  
from langchain_community.llms import Ollama
```
![](https://miro.medium.com/v2/resize:fit:889/0*eiQWHXXZgS0pHdi-.png)

## Step 2: Upload & Process PDFs

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
uploaded_file = st.file_uploader("Upload a PDF file", type="pdf")  if uploaded_file:  
    
    with open("temp.pdf", "wb") as f:  
        f.write(uploaded_file.getvalue())      
    loader = PDFPlumberLoader("temp.pdf")  
    docs = loader.load()
```

## Step 3: Chunk Documents Strategically

Leverage Streamlit’s file uploader to select a local PDF. Use `PDFPlumberLoader` to extract text efficiently without manual parsing.

```
text_splitter = SemanticChunker(HuggingFaceEmbeddings())  
documents = text_splitter.split_documents(docs)
```
![](https://miro.medium.com/v2/resize:fit:889/0*fkZuhUfDdVuWYAtY.png)

## Step 4: Create a Searchable Knowledge Base

Generate vector embeddings for the chunks and store them in a **FAISS index**.

- Embeddings allow fast, contextually relevant searches.

```
embeddings = HuggingFaceEmbeddings()  
vector_store = FAISS.from_documents(documents, embeddings)  
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  
```

## Step 5: Configure DeepSeek R1

Set up a **RetrievalQA chain** using the **DeepSeek R1 1.5B model**.

- This ensures answers are grounded in the PDF’s content rather than relying on the model’s training data.

```
llm = Ollama(model="deepseek-r1:1.5b")  
prompt = """  
1. Use ONLY the context below.  
2. If unsure, say "I don’t know".  
3. Keep answers under 4 sentences.  Context: {context}  Question: {question}  Answer:  
"""  
QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)
```

## Step 6: Assemble the RAG Chain

Integrate uploading, chunking, and retrieval into a cohesive pipeline.

- This approach gives the model verified context, enhancing accuracy.

```
llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)  
document_prompt = PromptTemplate(  
    template="Context:\ncontent:{page_content}\nsource:{source}",  
    input_variables=["page_content", "source"]  
)  
qa = RetrievalQA(  
    combine_documents_chain=StuffDocumentsChain(  
        llm_chain=llm_chain,  
        document_prompt=document_prompt  
    ),  
    retriever=retriever  
)
```

## Step 7: Launch the Web Interface

## Launch the Web Interface

Streamlit enables users to type questions and receive instant answers.

- Queries retrieve matching chunks, feed them to the model, and display results in real-time.

```
# Streamlit UI  
user_input = st.text_input("Ask your PDF a question:")  if user_input:  
    with st.spinner("Thinking..."):  
        response = qa(user_input)["result"]  
        st.write(response)
```
![](https://miro.medium.com/v2/resize:fit:889/0*Ho2aaFSPRviHFsrq.png)

You can find the complete code here: [https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js](https://gist.github.com/lisakim0/0204d7504d17cefceaf2d37261c1b7d5.js)

## The Future of RAG with DeepSeek

DeepSeek R1 is just the beginning. With upcoming features like **self-verification** and **multi-hop reasoning**, future RAG systems could debate and refine their logic autonomously.

Build your own RAG system today and unlock the full potential of document-based AI!

Source of this article: [How to Run Deepseek R1 Footer -->
    <footer class="main-footer">
        <div class="container">
            <p>&copy; 2024 日系解体工業株式会社 All Rights Reserved.</p>
            <p><a href="admin/login.php">管理者ログイン</a></p>
        </div>
    </footer>

    <script src="assets/js/main.js"></script>
</body>
</html>
